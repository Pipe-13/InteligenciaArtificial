{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFucbKQP3T5KMG2VG90lgK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pipe-13/InteligenciaArtificial/blob/main/NoticiasCategorias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qGUkxNcPHlDV"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25de979c"
      },
      "source": [
        "#%pip install gensim"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdCil1XlH6Cd",
        "outputId": "f0bde599-3e8a-48fe-dd6f-3aff535d63b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories = ['rec.sport.baseball', 'sci.electronics', 'comp.graphics', 'talk.politics.misc']\n",
        "newsgroups_data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "QbKOJVHQIAQ2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def cleanText(text):\n",
        "  # Tokenizar texto\n",
        "  tokens = gensim.utils.simple_preprocess(text, deacc=True)\n",
        "\n",
        "  # Eliminar stopwords y palabras cortas\n",
        "  processed_tokens = []\n",
        "  for token in tokens:\n",
        "    if token not in stop_words and len(token) > 2:\n",
        "      # Lemantizar\n",
        "      processed_tokens.append(lemmatizer.lemmatize(token))\n",
        "  return processed_tokens\n",
        "\n",
        "# Limpieza de noticias\n",
        "processed_docs = [cleanText(doc) for doc in newsgroups_data.data]"
      ],
      "metadata": {
        "id": "rBj93Qz7IBPZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creacion de diccionario y BoW\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
      ],
      "metadata": {
        "id": "DU7mLYCWIF2e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = len(categories)\n",
        "\n",
        "# Modelo LDA\n",
        "lda_model = LdaModel(corpus=corpus,\n",
        "                     id2word=dictionary,\n",
        "                     num_topics=4,\n",
        "                     random_state=43,\n",
        "                     chunksize=100)"
      ],
      "metadata": {
        "id": "nLKJiBRLIGq5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temas identificados por el modelo\n",
        "topics = lda_model.print_topics(num_words=10)\n",
        "for topic in topics:\n",
        "  print(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDXGnhV4II5S",
        "outputId": "614a9bf7-ec48-4380-a2e8-90d8d2de4461"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.012*\"one\" + 0.012*\"use\" + 0.011*\"line\" + 0.009*\"system\" + 0.009*\"would\" + 0.009*\"work\" + 0.009*\"problem\" + 0.008*\"circuit\" + 0.007*\"like\" + 0.007*\"need\"')\n",
            "(1, '0.020*\"year\" + 0.012*\"game\" + 0.010*\"run\" + 0.010*\"good\" + 0.009*\"one\" + 0.009*\"would\" + 0.009*\"last\" + 0.009*\"better\" + 0.008*\"think\" + 0.008*\"player\"')\n",
            "(2, '0.012*\"people\" + 0.012*\"would\" + 0.011*\"think\" + 0.009*\"government\" + 0.009*\"say\" + 0.009*\"president\" + 0.009*\"going\" + 0.009*\"state\" + 0.008*\"make\" + 0.007*\"american\"')\n",
            "(3, '0.014*\"graphic\" + 0.014*\"edu\" + 0.012*\"anyone\" + 0.011*\"thanks\" + 0.011*\"point\" + 0.010*\"file\" + 0.010*\"know\" + 0.009*\"program\" + 0.009*\"image\" + 0.009*\"would\"')\n"
          ]
        }
      ]
    }
  ]
}